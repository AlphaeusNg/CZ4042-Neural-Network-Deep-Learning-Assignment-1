{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFVxWZGJxprU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CS4001/4042 Assignment 1, Part B, Q2\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EycCozG06Duu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-widedeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lq0elU0J53Yo",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "\n",
    "import random\n",
    "random.seed(SEED)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU3xdVpwzuLx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_oYG6lNIh7Mp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "\n",
    "# TODO: Enter your code here\n",
    "df_train = df[df['year'] <= 2020]\n",
    "df_test = df[df['year'] >= 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_q9PoR50JAA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 linear layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 100 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZBY1iqUXtYWn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alpha\\anaconda3\\envs\\cz4042_helpme\\lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:334: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:10<00:00, 131.22it/s, loss=2.33e+5, metrics={'r2': -2.26}]  \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:10<00:00, 126.25it/s, loss=1.01e+5, metrics={'r2': 0.51}]  \n",
      "epoch 3: 100%|██████████| 1366/1366 [00:11<00:00, 120.68it/s, loss=8.71e+4, metrics={'r2': 0.6485}]\n",
      "epoch 4: 100%|██████████| 1366/1366 [00:12<00:00, 105.81it/s, loss=7.99e+4, metrics={'r2': 0.7116}]\n",
      "epoch 5: 100%|██████████| 1366/1366 [00:12<00:00, 110.14it/s, loss=7.58e+4, metrics={'r2': 0.7429}]\n",
      "epoch 6: 100%|██████████| 1366/1366 [00:13<00:00, 98.61it/s, loss=7.32e+4, metrics={'r2': 0.7619}] \n",
      "epoch 7: 100%|██████████| 1366/1366 [00:14<00:00, 92.55it/s, loss=7.14e+4, metrics={'r2': 0.7738}]\n",
      "epoch 8: 100%|██████████| 1366/1366 [00:15<00:00, 90.66it/s, loss=7.01e+4, metrics={'r2': 0.7809}]\n",
      "epoch 9: 100%|██████████| 1366/1366 [00:15<00:00, 89.72it/s, loss=6.92e+4, metrics={'r2': 0.7872}] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:14<00:00, 93.45it/s, loss=6.87e+4, metrics={'r2': 0.7903}]\n",
      "epoch 11: 100%|██████████| 1366/1366 [00:14<00:00, 92.14it/s, loss=6.77e+4, metrics={'r2': 0.7956}]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:17<00:00, 80.35it/s, loss=6.73e+4, metrics={'r2': 0.7978}]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:17<00:00, 77.01it/s, loss=6.69e+4, metrics={'r2': 0.8}]   \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:16<00:00, 85.29it/s, loss=6.65e+4, metrics={'r2': 0.8022}] \n",
      "epoch 15: 100%|██████████| 1366/1366 [00:11<00:00, 118.44it/s, loss=6.6e+4, metrics={'r2': 0.8051}] \n",
      "epoch 16: 100%|██████████| 1366/1366 [00:11<00:00, 119.32it/s, loss=6.58e+4, metrics={'r2': 0.8065}]\n",
      "epoch 17: 100%|██████████| 1366/1366 [00:14<00:00, 96.46it/s, loss=6.55e+4, metrics={'r2': 0.808}]  \n",
      "epoch 18: 100%|██████████| 1366/1366 [00:16<00:00, 84.55it/s, loss=6.5e+4, metrics={'r2': 0.8109}]  \n",
      "epoch 19: 100%|██████████| 1366/1366 [00:12<00:00, 106.76it/s, loss=6.49e+4, metrics={'r2': 0.8113}]\n",
      "epoch 20: 100%|██████████| 1366/1366 [00:11<00:00, 114.60it/s, loss=6.49e+4, metrics={'r2': 0.8111}]\n",
      "epoch 21: 100%|██████████| 1366/1366 [00:13<00:00, 101.57it/s, loss=6.45e+4, metrics={'r2': 0.8131}]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:13<00:00, 98.32it/s, loss=6.44e+4, metrics={'r2': 0.8138}] \n",
      "epoch 23: 100%|██████████| 1366/1366 [00:13<00:00, 105.02it/s, loss=6.41e+4, metrics={'r2': 0.8156}]\n",
      "epoch 24: 100%|██████████| 1366/1366 [00:12<00:00, 105.93it/s, loss=6.4e+4, metrics={'r2': 0.8159}] \n",
      "epoch 25: 100%|██████████| 1366/1366 [00:11<00:00, 116.01it/s, loss=6.39e+4, metrics={'r2': 0.8165}]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:11<00:00, 115.29it/s, loss=6.36e+4, metrics={'r2': 0.8181}]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:11<00:00, 119.92it/s, loss=6.35e+4, metrics={'r2': 0.819}] \n",
      "epoch 28: 100%|██████████| 1366/1366 [00:11<00:00, 117.64it/s, loss=6.34e+4, metrics={'r2': 0.8196}]\n",
      "epoch 29: 100%|██████████| 1366/1366 [00:11<00:00, 122.72it/s, loss=6.3e+4, metrics={'r2': 0.8211}] \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:11<00:00, 117.18it/s, loss=6.28e+4, metrics={'r2': 0.8231}]\n",
      "epoch 31: 100%|██████████| 1366/1366 [00:11<00:00, 119.74it/s, loss=6.3e+4, metrics={'r2': 0.8216}] \n",
      "epoch 32: 100%|██████████| 1366/1366 [00:10<00:00, 129.85it/s, loss=6.27e+4, metrics={'r2': 0.8226}]\n",
      "epoch 33: 100%|██████████| 1366/1366 [00:12<00:00, 110.93it/s, loss=6.26e+4, metrics={'r2': 0.8235}]\n",
      "epoch 34: 100%|██████████| 1366/1366 [00:17<00:00, 78.34it/s, loss=6.25e+4, metrics={'r2': 0.8238}] \n",
      "epoch 35: 100%|██████████| 1366/1366 [00:13<00:00, 100.49it/s, loss=6.23e+4, metrics={'r2': 0.825}] \n",
      "epoch 36: 100%|██████████| 1366/1366 [00:13<00:00, 99.40it/s, loss=6.22e+4, metrics={'r2': 0.8253}] \n",
      "epoch 37: 100%|██████████| 1366/1366 [00:13<00:00, 101.93it/s, loss=6.18e+4, metrics={'r2': 0.8276}]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:12<00:00, 111.67it/s, loss=6.16e+4, metrics={'r2': 0.8285}]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:12<00:00, 111.52it/s, loss=6.16e+4, metrics={'r2': 0.8283}]\n",
      "epoch 40: 100%|██████████| 1366/1366 [00:14<00:00, 97.08it/s, loss=6.15e+4, metrics={'r2': 0.8289}] \n",
      "epoch 41: 100%|██████████| 1366/1366 [00:12<00:00, 109.15it/s, loss=6.13e+4, metrics={'r2': 0.8299}]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:13<00:00, 102.60it/s, loss=6.13e+4, metrics={'r2': 0.8304}]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:14<00:00, 95.10it/s, loss=6.12e+4, metrics={'r2': 0.83}]   \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:14<00:00, 96.62it/s, loss=6.09e+4, metrics={'r2': 0.8321}] \n",
      "epoch 45: 100%|██████████| 1366/1366 [00:12<00:00, 109.26it/s, loss=6.07e+4, metrics={'r2': 0.8331}]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:12<00:00, 111.17it/s, loss=6.07e+4, metrics={'r2': 0.8332}]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:13<00:00, 99.66it/s, loss=6.08e+4, metrics={'r2': 0.8325}] \n",
      "epoch 48: 100%|██████████| 1366/1366 [00:13<00:00, 103.19it/s, loss=6.05e+4, metrics={'r2': 0.834}] \n",
      "epoch 49: 100%|██████████| 1366/1366 [00:13<00:00, 101.60it/s, loss=6.01e+4, metrics={'r2': 0.836}] \n",
      "epoch 50: 100%|██████████| 1366/1366 [00:12<00:00, 107.09it/s, loss=6.03e+4, metrics={'r2': 0.8352}]\n",
      "epoch 51: 100%|██████████| 1366/1366 [00:11<00:00, 120.56it/s, loss=5.98e+4, metrics={'r2': 0.8372}]\n",
      "epoch 52: 100%|██████████| 1366/1366 [00:14<00:00, 92.73it/s, loss=5.98e+4, metrics={'r2': 0.8378}]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:18<00:00, 74.65it/s, loss=5.97e+4, metrics={'r2': 0.8386}]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:19<00:00, 69.49it/s, loss=5.95e+4, metrics={'r2': 0.8393}]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:19<00:00, 70.01it/s, loss=5.94e+4, metrics={'r2': 0.8396}]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:19<00:00, 69.97it/s, loss=5.94e+4, metrics={'r2': 0.8397}]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:17<00:00, 80.23it/s, loss=5.93e+4, metrics={'r2': 0.8398}]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:16<00:00, 83.46it/s, loss=5.91e+4, metrics={'r2': 0.841}] \n",
      "epoch 59: 100%|██████████| 1366/1366 [00:16<00:00, 81.06it/s, loss=5.9e+4, metrics={'r2': 0.8415}] \n",
      "epoch 60: 100%|██████████| 1366/1366 [00:14<00:00, 93.43it/s, loss=5.89e+4, metrics={'r2': 0.842}]  \n",
      "epoch 61: 100%|██████████| 1366/1366 [00:12<00:00, 105.43it/s, loss=5.88e+4, metrics={'r2': 0.8424}]\n",
      "epoch 62: 100%|██████████| 1366/1366 [00:12<00:00, 112.72it/s, loss=5.87e+4, metrics={'r2': 0.8434}]\n",
      "epoch 63: 100%|██████████| 1366/1366 [00:11<00:00, 123.84it/s, loss=5.85e+4, metrics={'r2': 0.8441}]\n",
      "epoch 64: 100%|██████████| 1366/1366 [00:10<00:00, 126.21it/s, loss=5.83e+4, metrics={'r2': 0.8452}]\n",
      "epoch 65: 100%|██████████| 1366/1366 [00:11<00:00, 123.31it/s, loss=5.8e+4, metrics={'r2': 0.8468}] \n",
      "epoch 66: 100%|██████████| 1366/1366 [00:10<00:00, 126.35it/s, loss=5.77e+4, metrics={'r2': 0.8488}]\n",
      "epoch 67: 100%|██████████| 1366/1366 [00:10<00:00, 130.00it/s, loss=5.74e+4, metrics={'r2': 0.8503}]\n",
      "epoch 68: 100%|██████████| 1366/1366 [00:10<00:00, 133.52it/s, loss=5.7e+4, metrics={'r2': 0.8522}] \n",
      "epoch 69: 100%|██████████| 1366/1366 [00:10<00:00, 130.95it/s, loss=5.63e+4, metrics={'r2': 0.8558}]\n",
      "epoch 70: 100%|██████████| 1366/1366 [00:10<00:00, 133.05it/s, loss=5.56e+4, metrics={'r2': 0.8594}]\n",
      "epoch 71: 100%|██████████| 1366/1366 [00:10<00:00, 126.26it/s, loss=5.51e+4, metrics={'r2': 0.8621}]\n",
      "epoch 72: 100%|██████████| 1366/1366 [00:11<00:00, 123.82it/s, loss=5.42e+4, metrics={'r2': 0.8666}]\n",
      "epoch 73: 100%|██████████| 1366/1366 [00:11<00:00, 122.49it/s, loss=5.37e+4, metrics={'r2': 0.8689}]\n",
      "epoch 74: 100%|██████████| 1366/1366 [00:11<00:00, 122.99it/s, loss=5.29e+4, metrics={'r2': 0.8724}]\n",
      "epoch 75: 100%|██████████| 1366/1366 [00:11<00:00, 122.77it/s, loss=5.28e+4, metrics={'r2': 0.8734}]\n",
      "epoch 76: 100%|██████████| 1366/1366 [00:12<00:00, 110.33it/s, loss=5.24e+4, metrics={'r2': 0.8753}]\n",
      "epoch 77: 100%|██████████| 1366/1366 [00:12<00:00, 110.72it/s, loss=5.23e+4, metrics={'r2': 0.8755}]\n",
      "epoch 78: 100%|██████████| 1366/1366 [00:12<00:00, 109.83it/s, loss=5.21e+4, metrics={'r2': 0.8767}]\n",
      "epoch 79: 100%|██████████| 1366/1366 [00:11<00:00, 114.37it/s, loss=5.2e+4, metrics={'r2': 0.8772}] \n",
      "epoch 80: 100%|██████████| 1366/1366 [00:11<00:00, 114.52it/s, loss=5.19e+4, metrics={'r2': 0.8775}]\n",
      "epoch 81: 100%|██████████| 1366/1366 [00:13<00:00, 104.12it/s, loss=5.17e+4, metrics={'r2': 0.8782}]\n",
      "epoch 82: 100%|██████████| 1366/1366 [00:12<00:00, 110.72it/s, loss=5.15e+4, metrics={'r2': 0.8794}]\n",
      "epoch 83: 100%|██████████| 1366/1366 [00:12<00:00, 110.85it/s, loss=5.15e+4, metrics={'r2': 0.8791}]\n",
      "epoch 84: 100%|██████████| 1366/1366 [00:11<00:00, 118.04it/s, loss=5.14e+4, metrics={'r2': 0.8799}]\n",
      "epoch 85: 100%|██████████| 1366/1366 [00:11<00:00, 114.57it/s, loss=5.13e+4, metrics={'r2': 0.8803}]\n",
      "epoch 86: 100%|██████████| 1366/1366 [00:11<00:00, 113.95it/s, loss=5.12e+4, metrics={'r2': 0.8808}]\n",
      "epoch 87: 100%|██████████| 1366/1366 [00:11<00:00, 119.38it/s, loss=5.11e+4, metrics={'r2': 0.8814}]\n",
      "epoch 88: 100%|██████████| 1366/1366 [00:11<00:00, 119.69it/s, loss=5.08e+4, metrics={'r2': 0.8824}]\n",
      "epoch 89: 100%|██████████| 1366/1366 [00:11<00:00, 115.37it/s, loss=5.08e+4, metrics={'r2': 0.8828}]\n",
      "epoch 90: 100%|██████████| 1366/1366 [00:11<00:00, 116.31it/s, loss=5.08e+4, metrics={'r2': 0.8825}]\n",
      "epoch 91: 100%|██████████| 1366/1366 [00:11<00:00, 116.30it/s, loss=5.06e+4, metrics={'r2': 0.8838}]\n",
      "epoch 92: 100%|██████████| 1366/1366 [00:11<00:00, 117.96it/s, loss=5.06e+4, metrics={'r2': 0.8832}]\n",
      "epoch 93: 100%|██████████| 1366/1366 [00:11<00:00, 120.37it/s, loss=5.05e+4, metrics={'r2': 0.8839}]\n",
      "epoch 94: 100%|██████████| 1366/1366 [00:10<00:00, 125.24it/s, loss=5.02e+4, metrics={'r2': 0.8853}]\n",
      "epoch 95: 100%|██████████| 1366/1366 [00:11<00:00, 122.64it/s, loss=5.03e+4, metrics={'r2': 0.8847}]\n",
      "epoch 96: 100%|██████████| 1366/1366 [00:10<00:00, 125.92it/s, loss=5.04e+4, metrics={'r2': 0.8843}]\n",
      "epoch 97: 100%|██████████| 1366/1366 [00:11<00:00, 121.22it/s, loss=5.02e+4, metrics={'r2': 0.8852}]\n",
      "epoch 98: 100%|██████████| 1366/1366 [00:11<00:00, 123.31it/s, loss=5.02e+4, metrics={'r2': 0.8854}]\n",
      "epoch 99: 100%|██████████| 1366/1366 [00:10<00:00, 125.96it/s, loss=5e+4, metrics={'r2': 0.8864}]   \n",
      "epoch 100: 100%|██████████| 1366/1366 [00:10<00:00, 132.24it/s, loss=5.02e+4, metrics={'r2': 0.8853}]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "categorical_columns = ['month', 'town', 'flat_model_type', 'storey_range']\n",
    "continuous_columns = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "target = df_train['resale_price'].values\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=categorical_columns, continuous_cols=continuous_columns\n",
    ")\n",
    "\n",
    "X_tab = tab_preprocessor.fit_transform(df_train)\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_columns,\n",
    "    mlp_hidden_dims=[200, 100]\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "trainer = Trainer(model, cost_function=\"root_mean_squared_error\", metrics=[R2Score], num_workers=0)\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V46s-MdM0y5c",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "KAhAgvMC07g6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:02<00:00, 401.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 97423.07847184621\n",
      "R2: 0.6157308570297755\n"
     ]
    }
   ],
   "source": [
    "# TODO: Enter your code here\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_test_transformed  = tab_preprocessor.transform(df_test)\n",
    "y_pred = trainer.predict(X_tab=X_test_transformed , batch_size=64)\n",
    "\n",
    "true_values  = df_test['resale_price'].values\n",
    "rmse = mean_squared_error(y_pred, true_values, squared=False)\n",
    "\n",
    "r_squared  = r2_score(y_pred, true_values)\n",
    "\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"R2: {r_squared }\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "cz4042_b",
   "language": "python",
   "display_name": "cz4042_B"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}